{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Задача 5 (бонусная): экспериментальное исследование алгоритмов для регулярных запросов"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Подготовка окружения\n",
    "\n",
    "Для исполнения кода, приведённого в данном исследовании, необходима система с ядром Linux, подготовленная при помощи выполнения команд, приведённых далее."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "!pip install -r ../requirements.txt\n",
    "!pip install pycubool\n",
    "# @formatter:on"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Введение\n",
    "\n",
    "В данном работе рассматривается задача достижимости в графе с регулярными ограничениями и исследуется производительность её решений матричными подходами на языке Python при помощи библиотек, осуществляющих матричные операции на CPU и GPU."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### О задаче достижимости\n",
    "\n",
    "Пусть дан конечный ориентированный помеченный граф $G = (V, E, L)$, в котором каждому пути $\\pi \\in E^*$, состоящему из рёбер с метками $l_1, ..., l_n \\in L$, сопоставляется слово по правилу $\\omega(\\pi) = l_1 * ... * l_n$, где $*$ --- конкатенация.\n",
    "\n",
    "Пусть также дано регулярное выражение $R$ для языка над алфавитом $L$.\n",
    "\n",
    "В данных условиях мы будем рассматривать три варианта задачи о достижимости в графе $G$ с регулярными ограничениями, заданным $R$.\n",
    "\n",
    "#### Постановка 1. Достижимость между всеми парами вершин.\n",
    "\n",
    "Данная постановка задачи является наиболее общей.\n",
    "\n",
    "В ней задача заключается в нахождении таких пар вершин $G$, что между ними существует путь, которому сопоставляется слово из языка, задаваемого $R$. То есть задача заключается в нахождении множества $\\{(v_i, v_j) \\in V^2: \\exists \\pi \\in E^*: start(\\pi) = v_i, final(\\pi) = v_j, \\omega(\\pi) \\in language(R)\\}$.\n",
    "\n",
    "#### Постановка 2. Достижимость для всего множества заданных вершин.\n",
    "\n",
    "В данной постановке задачи из множества $V$ всех вершин графа дополнительно выделяются подмножества стартовых и финальных вершин: $V_S \\subseteq V$ и $V_F \\subseteq V$ соответственно. Задача заключается в нахождении всех таких финальных вершин, таких, что до них существуют пути из стартовых, которым сопоставляются слова из языка, задаваемого $R$. То есть задача заключается в нахождении множества $\\{v_i \\in V_F: \\exists \\pi \\in E^*: start(\\pi) \\in V_S, final(\\pi) = v_j, \\omega(\\pi) \\in language(R)\\}$.\n",
    "\n",
    "#### Постановка 3. Достижимость для каждой вершины из заданного множества стартовых вершин.\n",
    "\n",
    "Данная постановка является комбинацией предыдущих двух. Здесь также выделяются множества стартовых и финальных вершин $V_S и $V_F, в которых найденные пути должны начинаться и оканчиваться соответственно, однако, аналогично постановке 1, требуется найти множество пар вершин-начал и вершин-концов данных путей. То есть задача заключается в нахождении множества $\\{(v_i, v_j) \\in V_S \\times V_F: \\exists \\pi \\in E^*: start(\\pi) = v_i, final(\\pi) = v_j, \\omega(\\pi) \\in language(R)\\}$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Исследуемые решения\n",
    "\n",
    "В данной работе для решения поставленных выше задач используются методы, кратко описываемые следующим образом:\n",
    "\n",
    "1. **Достижимость между всеми парами вершин**. Через тензорное произведение строится пересечение конечных автоматов, соответствующих графу и регулярному выражению, для которого затем считается транзитивное замыкание через матричное возведение в степень.\n",
    "2. **Достижимость для всего множества заданных вершин**. Алгоритм решения основывается на синхронном обходе в ширину конечных автоматов, соответствующих регулярному выражению и графу. Обход проводится через умножение фронта, представленного как матрица условно разделенная на соответствующие две части, на матрицу смежности, построенную как прямая сумма матриц смежности обоих автоматов. Матрица фронта в левой части представляет из себя единичную матрицу, а в правой --- строки-фронты-обхода, возможные для соответствующей вершины в левой части. После каждого умножения матрица фронта трансформируется для поддержки данного инварианта.\n",
    "3. **Достижимость для каждой вершины из заданного множества стартовых вершин**. Принцип решения схож с предыдущим пунктом, однако фронт разделяется для каждой стартовой вершины и представляется как единая матрица, что позволяет хранить информацию о стартовой вершине для каждой достигнутой финальной.\n",
    "\n",
    "На практике матрицы смежности, используемые в представленных алгоритмах являются сильно разреженными, поэтому при их реализации становится выгодно представлять такие матрицы специальным образом, избегая хранения \"пустых\" ячеек. Каждый из описанных алгоритмов был реализован на языке Python при помощи двух библиотек, содержащих таких типы матриц:\n",
    "\n",
    "1. [**scipy.sparse**](https://docs.scipy.org/doc/scipy/reference/sparse.html) --- модуль SciPy, предоставляющий несколько видов двухмерных разреженных массивов с интерфейсом взаимодействия, приближенным к таковому в NumPy. Все операции, проводимые с массивами, происходят на CPU.\n",
    "2. [**pycubool**](https://github.com/JetBrains-Research/cuBool) --- обертка для C-библиотеки, позволяющая проводить операции линейной алгебры над разреженными матрицами на GPU марки NVIDIA при помощи технологии CUDA.\n",
    "\n",
    "Оба способа реализации максимально идентичны (на сколько это позволяют интерфейсы представленных библиотек)."
   ],
   "metadata": {
    "collapsed": false
   },
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Реализация на scipy.sparse\n",
    "\n",
    "Реализация алгоритмов, основанная на `scipy.sparse`, расположена в модуле [`project.rpq`](../project/rpq.py) данного репозитория."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from project.rpq import BfsMode\n",
    "from project.rpq import rpq_by_bfs\n",
    "from project.rpq import rpq_by_tensor"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Реализация на pycubool\n",
    "\n",
    "Реализация алгоритмов, основанная на `pycubool`, представленная ниже:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from typing import Any\n",
    "from typing import NamedTuple\n",
    "\n",
    "import pycubool as cb\n",
    "from pyformlang.finite_automaton import EpsilonNFA\n",
    "\n",
    "\n",
    "class BoolDecompositionCuda:\n",
    "    class StateInfo(NamedTuple):\n",
    "        data: Any\n",
    "        is_start: bool\n",
    "        is_final: bool\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        states: list[StateInfo] | None = None,\n",
    "        adjs: dict[Any, cb.Matrix] | None = None,\n",
    "    ):\n",
    "        self.states: list[BoolDecompositionCuda.StateInfo] = (\n",
    "            states if states is not None else []\n",
    "        )\n",
    "        self.adjs: dict[Any, cb.Matrix] = adjs if adjs is not None else {}\n",
    "\n",
    "    @classmethod\n",
    "    def from_nfa(\n",
    "        cls, nfa: EpsilonNFA, sort_states: bool = False\n",
    "    ) -> \"BoolDecompositionCuda\":\n",
    "        # Construct states, removing duplicates\n",
    "        states = list(\n",
    "            set(\n",
    "                cls.StateInfo(\n",
    "                    data=st.value,\n",
    "                    is_start=st in nfa.start_states,\n",
    "                    is_final=st in nfa.final_states,\n",
    "                )\n",
    "                for st in nfa.states\n",
    "            )\n",
    "        )\n",
    "        if sort_states:\n",
    "            states = sorted(states, key=lambda st: st.data)\n",
    "\n",
    "        # Construct adjacency matrices\n",
    "        adjs = {}\n",
    "        transitions = nfa.to_dict()\n",
    "        n = len(states)\n",
    "        for n_from in transitions:\n",
    "            for symbol, ns_to in transitions[n_from].items():\n",
    "                adj = adjs.setdefault(symbol.value, cb.Matrix.empty((n, n)))\n",
    "                beg_index = next(i for i, s in enumerate(states) if s.data == n_from)\n",
    "                for n_to in ns_to if isinstance(ns_to, set) else {ns_to}:\n",
    "                    end_index = next(i for i, s in enumerate(states) if s.data == n_to)\n",
    "                    adj[beg_index, end_index] = True\n",
    "\n",
    "        return cls(states, adjs)\n",
    "\n",
    "    def intersect(self, other: \"BoolDecompositionCuda\") -> \"BoolDecompositionCuda\":\n",
    "        # Set of states of the intersection is the product of the given sets of states\n",
    "        # State is \"start\" if both of its element states are \"start\"\n",
    "        # State is \"final\" if both of its element states are \"final\"\n",
    "        states = [\n",
    "            self.StateInfo(\n",
    "                data=(st1.data, st2.data),\n",
    "                is_start=st1.is_start and st2.is_start,\n",
    "                is_final=st1.is_final and st2.is_final,\n",
    "            )\n",
    "            for st1, st2 in product(self.states, other.states)\n",
    "        ]\n",
    "\n",
    "        # Set of symbols of the intersection is the union of the given sets of symbols\n",
    "        # Adjacency matrix for a symbol is a kronecker product of the given matrices of\n",
    "        # this symbol\n",
    "        adjs = {}\n",
    "        n = len(states)\n",
    "        for symbol in set(self.adjs.keys()).union(set(other.adjs.keys())):\n",
    "            if symbol in self.adjs and symbol in other.adjs:\n",
    "                adjs[symbol] = self.adjs[symbol].kronecker(other.adjs[symbol])\n",
    "            else:\n",
    "                adjs[symbol] = cb.Matrix.empty((n, n))\n",
    "\n",
    "        return BoolDecompositionCuda(states, adjs)\n",
    "\n",
    "    def transitive_closure_any_symbol(self) -> tuple[list[int], list[int]]:\n",
    "        # Gather all matrices to get all existing paths\n",
    "        n = len(self.states)\n",
    "        adj_all = cb.Matrix.empty((n, n))\n",
    "        for adj in self.adjs.values():\n",
    "            adj_all = adj_all.ewiseadd(adj)\n",
    "\n",
    "        # Transitive closure by repeated-squaring-like approach\n",
    "        while True:\n",
    "            # nvals gives the number of paths\n",
    "            prev_path_num = adj_all.nvals\n",
    "            # Multiplication gives new paths, while sum retains the old ones\n",
    "            adj_all.mxm(adj_all, out=adj_all, accumulate=True)\n",
    "            # If no new paths appear, all paths have been discovered\n",
    "            if prev_path_num == adj_all.nvals:\n",
    "                break\n",
    "\n",
    "        # Convert to a more user-friendly representation\n",
    "        return adj_all.to_lists()\n",
    "\n",
    "    def _direct_sum(self, other: \"BoolDecompositionCuda\") -> \"BoolDecompositionCuda\":\n",
    "        states = self.states + other.states\n",
    "\n",
    "        adjs = {}\n",
    "        for symbol in set(self.adjs.keys()).intersection(set(other.adjs.keys())):\n",
    "            dsum = cb.Matrix.empty((len(states), len(states)))\n",
    "            for i, j in self.adjs[symbol]:\n",
    "                dsum[i, j] = True\n",
    "            for i, j in other.adjs[symbol]:\n",
    "                dsum[len(self.states) + i, len(self.states) + j] = True\n",
    "            adjs[symbol] = dsum\n",
    "\n",
    "        return BoolDecompositionCuda(states, adjs)\n",
    "\n",
    "    def constrained_bfs(\n",
    "        self, constraint: \"BoolDecompositionCuda\", separated: bool = False\n",
    "    ) -> set[int] | set[tuple[int, int]]:\n",
    "        # Save states number because will use them heavily for matrix construction\n",
    "        n = len(constraint.states)\n",
    "        m = len(self.states)\n",
    "\n",
    "        direct_sum = constraint._direct_sum(self)\n",
    "\n",
    "        # Create initial front from starts of constraint (left) and self (right)\n",
    "        start_states_indices = [i for i, st in enumerate(self.states) if st.is_start]\n",
    "        init_front = (\n",
    "            _init_bfs_front(self.states, constraint.states)\n",
    "            if not separated\n",
    "            else _init_separated_bfs_front(\n",
    "                self.states, constraint.states, start_states_indices\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Create visited, fill with zeroes instead of init_front to get rid of initial\n",
    "        # positions in the result\n",
    "        visited = cb.Matrix.empty(init_front.shape)\n",
    "\n",
    "        # Perform matrix-multiplication-based-BFS until visited stops changing\n",
    "        while True:\n",
    "            old_visited_nvals = visited.nvals\n",
    "\n",
    "            # Perform a BFS step for each matrix in direct sum\n",
    "            for _, adj in direct_sum.adjs.items():\n",
    "                # Compute new front for the symbol\n",
    "                front_part = (\n",
    "                    visited.mxm(adj) if init_front is None else init_front.mxm(adj)\n",
    "                )\n",
    "                # Transform the resulting front so that:\n",
    "                # 1. It only contains rows with non-zeros in both parts.\n",
    "                # 2. Its left part contains non-zeroes only on its main diagonal.\n",
    "                visited = visited.ewiseadd(_transform_front_part(front_part, n))\n",
    "\n",
    "            # Can use visited instead now\n",
    "            init_front = None\n",
    "\n",
    "            # If no new non-zero elements have appeared, we've visited all we can\n",
    "            if visited.nvals == old_visited_nvals:\n",
    "                break\n",
    "\n",
    "        # If visited a final self-state in final constraint-state, we found a result\n",
    "        results = set()\n",
    "        for i, j in visited.to_list():\n",
    "            # Check that the element is from the self part (which is the main BFS part)\n",
    "            # and the final state requirements are satisfied\n",
    "            if j >= n and constraint.states[i % n].is_final:  # % is for separated BFS\n",
    "                self_st_index = j - n\n",
    "                if self.states[self_st_index].is_final:\n",
    "                    results.add(\n",
    "                        self_st_index\n",
    "                        if not separated\n",
    "                        else (start_states_indices[i // n], self_st_index)\n",
    "                    )\n",
    "        return results\n",
    "\n",
    "\n",
    "def _init_bfs_front(\n",
    "    self_states: list[BoolDecompositionCuda.StateInfo],\n",
    "    constr_states: list[BoolDecompositionCuda.StateInfo],\n",
    "    self_start_indices: list[int] | None = None,\n",
    ") -> cb.Matrix:\n",
    "    front = cb.Matrix.empty((len(constr_states), len(constr_states) + len(self_states)))\n",
    "\n",
    "    if self_start_indices is None:\n",
    "        self_start_indices = [j for j, st in enumerate(self_states) if st.is_start]\n",
    "\n",
    "    for i, st in enumerate(constr_states):\n",
    "        if st.is_start:\n",
    "            front[i, i] = True  # Mark diagonal element as start\n",
    "            for j in self_start_indices:\n",
    "                front[i, len(constr_states) + j] = True  # Fill start row\n",
    "\n",
    "    return front\n",
    "\n",
    "\n",
    "def _init_separated_bfs_front(\n",
    "    self_states: list[BoolDecompositionCuda.StateInfo],\n",
    "    constr_states: list[BoolDecompositionCuda.StateInfo],\n",
    "    start_states_indices: list[int],\n",
    ") -> cb.Matrix:\n",
    "    fronts = [\n",
    "        _init_bfs_front(\n",
    "            self_states,\n",
    "            constr_states,\n",
    "            self_start_indices=[st_i],\n",
    "        )\n",
    "        for st_i in start_states_indices\n",
    "    ]\n",
    "\n",
    "    if len(fronts) == 0:\n",
    "        return cb.Matrix.empty(\n",
    "            (len(constr_states), len(constr_states) + len(self_states))\n",
    "        )\n",
    "\n",
    "    # Build the united front as a vertical stack of the separated fronts\n",
    "    result = cb.Matrix.empty(\n",
    "        (len(fronts) * len(constr_states), len(constr_states) + len(self_states))\n",
    "    )\n",
    "    vstack_helper = cb.Matrix.empty((len(fronts), 1))\n",
    "    for i, front in enumerate(fronts):\n",
    "        vstack_helper.build(rows={i}, cols={0})\n",
    "        result = result.ewiseadd(vstack_helper.kronecker(front))\n",
    "    return result\n",
    "\n",
    "\n",
    "def _transform_front_part(front_part: cb.Matrix, constr_states_num: int) -> cb.Matrix:\n",
    "    transformed_front_part = cb.Matrix.empty(front_part.shape)\n",
    "    # Perform the transformation by rows\n",
    "    for i, j in front_part.to_list():\n",
    "        # If the element is from the constraint part\n",
    "        if j < constr_states_num:\n",
    "            non_zero_row_right = front_part[i: i + 1, constr_states_num:]\n",
    "            # If the right part contains non-zero elements\n",
    "            if non_zero_row_right.nvals > 0:\n",
    "                # Account for separated front\n",
    "                row_shift = i // constr_states_num * constr_states_num\n",
    "                # Mark the row in the left part\n",
    "                transformed_front_part[row_shift + j, j] = True\n",
    "                # Update right part of the row\n",
    "                for _, r_j in non_zero_row_right:\n",
    "                    transformed_front_part[\n",
    "                        row_shift + j, constr_states_num + r_j\n",
    "                    ] = True\n",
    "    return transformed_front_part\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import pyformlang.regular_expression as re\n",
    "from typing import TypeVar\n",
    "\n",
    "from project.automata_utils import graph_to_nfa\n",
    "from project.automata_utils import regex_to_min_dfa\n",
    "\n",
    "_NodeType = TypeVar(\"_NodeType\")\n",
    "\n",
    "\n",
    "def rpq_by_tensor_cuda(\n",
    "    graph: nx.Graph,\n",
    "    query: str | re.Regex,\n",
    "    starts: set[_NodeType] | None = None,\n",
    "    finals: set[_NodeType] | None = None,\n",
    ") -> set[tuple[_NodeType, _NodeType]]:\n",
    "    # Create boolean decompositions for the graph and the query\n",
    "    graph_decomp = BoolDecompositionCuda.from_nfa(graph_to_nfa(graph, starts, finals))\n",
    "    query_decomp = BoolDecompositionCuda.from_nfa(regex_to_min_dfa(query))\n",
    "\n",
    "    # Intersection of decompositions gives intersection of languages\n",
    "    intersection = graph_decomp.intersect(query_decomp)\n",
    "    # Transitive closure helps determine reachability\n",
    "    transitive_closure_indices = intersection.transitive_closure_any_symbol()\n",
    "\n",
    "    # Two nodes satisfy the query if one is the beginning of a path (i.e. a word) and\n",
    "    # the other is its end\n",
    "    results = set()\n",
    "    for n_from_i, n_to_i in zip(*transitive_closure_indices):\n",
    "        n_from = intersection.states[n_from_i]\n",
    "        n_to = intersection.states[n_to_i]\n",
    "        if n_from.is_start and n_to.is_final:\n",
    "            beg_graph_node = n_from.data[0]\n",
    "            end_graph_node = n_to.data[0]\n",
    "            results.add((beg_graph_node, end_graph_node))\n",
    "    return results\n",
    "\n",
    "\n",
    "def rpq_by_bfs_cuda(\n",
    "    graph: nx.Graph,\n",
    "    query: str | re.Regex,\n",
    "    starts: set[_NodeType] | None = None,\n",
    "    finals: set[_NodeType] | None = None,\n",
    "    mode: BfsMode = BfsMode.FIND_COMMON_REACHABLE_SET,\n",
    ") -> set[_NodeType] | set[tuple[_NodeType, _NodeType]]:\n",
    "    graph_decomp = BoolDecompositionCuda.from_nfa(graph_to_nfa(graph, starts, finals))\n",
    "    query_decomp = BoolDecompositionCuda.from_nfa(regex_to_min_dfa(query))\n",
    "\n",
    "    result_indices = graph_decomp.constrained_bfs(\n",
    "        query_decomp, separated=mode == BfsMode.FIND_REACHABLE_FOR_EACH_START\n",
    "    )\n",
    "\n",
    "    match mode:\n",
    "        case BfsMode.FIND_COMMON_REACHABLE_SET:\n",
    "            return {graph_decomp.states[i].data for i in result_indices}\n",
    "        case BfsMode.FIND_REACHABLE_FOR_EACH_START:\n",
    "            return {\n",
    "                (graph_decomp.states[i].data, graph_decomp.states[j].data)\n",
    "                for i, j in result_indices\n",
    "            }\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Цель работы\n",
    "\n",
    "Целью данной работы является анализ производительности представленных решений задачи достижимости в графе с регулярными ограничениями в трёх её постановках, описанных выше, а именно, ответ на следующие три вопроса:\n",
    "\n",
    "1. При каких условиях использование `pycubool` даёт выигрыш в производительности по сравнению с `scipy.sparse`? Для каждой из трёх постановок задачи.\n",
    "2. При использовании `pycubool`, начиная с какого размера стартового множества выгоднее решать задачу для всех пар и выбирать нужные?\n",
    "3. При использовании `pycubool`, насколько решение задачи во второй её постановке быстрее решения в третьей при одинаковых начальных условиях?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Условия эксперимента\n",
    "\n",
    "В данной секции описывается каким образом проводился эксперимент, соответствующий цели данной работы."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Характеристики оборудование\n",
    "\n",
    "Оборудование, на котором проводились замеры, обладает следующими характеристиками:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# @formatter:off\n",
    "\n",
    "! echo '----- OS  -----'\n",
    "! hostnamectl | grep -E 'Operating System|Kernel'\n",
    "\n",
    "! echo '----- CPU -----'\n",
    "! lscpu | grep -E 'Architecture|Model name|Thread\\(s\\)|Core\\(s\\)|Socket\\(s\\)|MHz|cache'\n",
    "\n",
    "! echo '----- GPU -----'\n",
    "! nvidia-smi -L\n",
    "! nvidia-smi | grep CUDA\n",
    "\n",
    "! echo '----- RAM -----'\n",
    "! free -m\n",
    "\n",
    "# @formatter:on"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Описание использованного набора данных\n",
    "\n",
    "Для проведения экспериментов необходимо было выбрать графы для поиска путей и регулярные выражения для задания ограничений."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Выбор графов\n",
    "\n",
    "Эксперимент проводился на следующих десяти графах из [датасета `CFPQ_Data`](https://jetbrains-research.github.io/CFPQ_Data/dataset/index.html):\n",
    "\n",
    "- `skos`, `atom`, `bzip`, `pr`, `ls`, `pathways`, `enzyme`, `apache` --- как репрезентативные по количеству вершин и рёбер относительно всего датасета\n",
    "- `go_hierarchy`, `geospecies` --- как имеющие сравнительно высокое отношение числа рёбер к числу вершин\n",
    "\n",
    "Графы с числом вершин или рёбер, приближающимся к трём миллионам и выше, решено было не рассматривать, так как время их обработки оказалось достаточно велико."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from project.graph_utils import load_graph_from_cfpq_data\n",
    "\n",
    "GRAPHS = []\n",
    "\n",
    "\n",
    "def init_graphs():\n",
    "    for name in [\n",
    "        \"skos\",\n",
    "        \"atom\",\n",
    "        \"bzip\",\n",
    "        \"pr\",\n",
    "        \"ls\",\n",
    "        \"pathways\",\n",
    "        \"enzyme\",\n",
    "        \"go_hierarchy\",\n",
    "        \"apache\",\n",
    "        \"geospecies\"\n",
    "    ]:\n",
    "        graph = load_graph_from_cfpq_data(name)\n",
    "        graph.name = name\n",
    "        GRAPHS.append(graph)\n",
    "\n",
    "\n",
    "init_graphs()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ниже приведена статистика по выбранным графам."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from project.graph_utils import get_graph_info\n",
    "\n",
    "\n",
    "def get_most_common(source: list[str], n: int) -> list[tuple[str, int]]:\n",
    "    return sorted(Counter(source).most_common(n), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "\n",
    "def graph_stats() -> pd.DataFrame:\n",
    "    stats = []\n",
    "\n",
    "    for graph in GRAPHS:\n",
    "        info = get_graph_info(graph)\n",
    "        stats.append([\n",
    "            graph.name,\n",
    "            info.nodes_num,\n",
    "            info.edges_num,\n",
    "            info.edges_num / info.nodes_num,\n",
    "            get_most_common(info.labels, 4),\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(\n",
    "        sorted(stats, key=lambda st: st[2]),\n",
    "        columns=[\"Name\", \"Nodes\", \"Edges\", \"Edges per node\", \"Labels (most common -> least common)\"],\n",
    "    )\n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "graph_stats()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Выбор регулярных выражений\n",
    "\n",
    "Для каждого графа создавалось по четыре регулярных выражения следующих видов:\n",
    "\n",
    "1. $R_1 = l_1 \\ | \\ l_2$ --- одно из наиболее простых выражений.\n",
    "2. $R_2 = (l_1 \\ | \\ l_2) \\ (l_3 \\ | \\ l_4)$ --- более сложное выражение.\n",
    "3. $R_3 = l_1 \\ l_2 \\ l_3 \\ l_4$ --- выражение со сравнительно большим числом состояний и малым числом переходов.\n",
    "4. $R_4 = (l_1 \\ | \\ l_2)^* \\ l_3$ --- выражение со сравнительно большим числом переходов и малым числом состояний.\n",
    "\n",
    "В качестве меток $l_1, l_2, l_3, l_4$ выбирались четыре наиболее часто встречающиеся в конкретном графе метки в порядке убывания количества рёбер с ними (а если некоторые метки появляются на одинаковом количестве рёбер, то они сортируются в обратном алфавитном порядке). Если в графе менее четырёх меток, то существующие метки брались повторно в том же порядке необходимое количество раз. То есть, если граф обладает метками `ab` и `ac` на трёх рёбрах и `b` --- на одном, то $l_1 =$`ac`, $l_2 =$`ab`, $l_3 =$`b`, $l_4 =$`ac`.\n",
    "\n",
    "Далее представлен код для генерации описанных регулярных выражений."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pyformlang.finite_automaton as fa\n",
    "\n",
    "MAX_REGEX_LABELS_NUM = 4\n",
    "\n",
    "\n",
    "def label_to_regexp(l: str) -> re.Regex:\n",
    "    re0 = re.Regex(\"\")\n",
    "    re0.head = fa.Symbol(l)\n",
    "    return re0\n",
    "\n",
    "\n",
    "def r1(l1: str, l2: str) -> re.Regex:\n",
    "    re1 = label_to_regexp(l1)\n",
    "    re2 = label_to_regexp(l2)\n",
    "    return re1.union(re2)\n",
    "\n",
    "\n",
    "def r2(l1: str, l2: str, l3: str, l4: str) -> re.Regex:\n",
    "    re1 = label_to_regexp(l1)\n",
    "    re2 = label_to_regexp(l2)\n",
    "    re12 = re1.union(re2)\n",
    "\n",
    "    re3 = label_to_regexp(l3)\n",
    "    re4 = label_to_regexp(l4)\n",
    "    re34 = re3.union(re4)\n",
    "\n",
    "    return re12.concatenate(re34)\n",
    "\n",
    "\n",
    "def r3(l1: str, l2: str, l3: str, l4: str) -> re.Regex:\n",
    "    re1 = label_to_regexp(l1)\n",
    "    re2 = label_to_regexp(l2)\n",
    "    re3 = label_to_regexp(l3)\n",
    "    re4 = label_to_regexp(l4)\n",
    "    return re1.concatenate(re2).concatenate(re3).concatenate(re4)\n",
    "\n",
    "\n",
    "def r4(l1: str, l2: str, l3: str, l4: str) -> re.Regex:\n",
    "    re1 = label_to_regexp(l1)\n",
    "    re2 = label_to_regexp(l2)\n",
    "    re3 = label_to_regexp(l3)\n",
    "    re4 = label_to_regexp(l4)\n",
    "    re1234 = re1.union(re2).union(re3).union(re4)\n",
    "    re1234 = re1234.kleene_star()\n",
    "\n",
    "    return re1234.concatenate(re3).concatenate(re4)\n",
    "\n",
    "\n",
    "def create_regexes_for_graph(graph: nx.Graph) -> list[re.Regex]:\n",
    "    info = get_graph_info(graph)\n",
    "    labels = get_most_common(info.labels, MAX_REGEX_LABELS_NUM)\n",
    "    while len(labels) < MAX_REGEX_LABELS_NUM:\n",
    "        labels += labels\n",
    "    labels = labels[:MAX_REGEX_LABELS_NUM]\n",
    "\n",
    "    return [\n",
    "        r1(labels[0], labels[1]),\n",
    "        r2(*labels),\n",
    "        r3(*labels),\n",
    "        r4(*labels)\n",
    "    ]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Также ниже приведены характеристики данных регулярных выражений как конечных автоматов, к которым они будут сводиться в исследуемых алгоритмах."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def regex_stats() -> pd.DataFrame:\n",
    "    stats = []\n",
    "\n",
    "    for name, r in [\n",
    "        (\"r1\", r1(\"a\", \"b\")),\n",
    "        (\"r2\", r2(\"a\", \"b\", \"c\", \"d\")),\n",
    "        (\"r3\", r3(\"a\", \"b\", \"c\", \"d\")),\n",
    "        (\"r4\", r4(\"a\", \"b\", \"c\", \"d\")),\n",
    "    ]:\n",
    "        dfa = regex_to_min_dfa(r)\n",
    "        stats.append([\n",
    "            name,\n",
    "            len(dfa.symbols),\n",
    "            len(dfa.states),\n",
    "            dfa.get_number_transitions(),\n",
    "            dfa.get_number_transitions() / len(dfa.states),\n",
    "        ])\n",
    "\n",
    "    return pd.DataFrame(stats, columns=[\"Name\", \"Symbols\", \"States\", \"Transitions\", \"Transitions per state\"])\n",
    "\n",
    "\n",
    "regex_stats()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Выбор множеств стартовых и финальных вершин\n",
    "\n",
    "Стоит отметить, что множество стартовых вершин может заметно и нетривиально повлиять на быстродействие алгоритмов, решающих задачу в постановках 2 и 3, так как если из выбранных вершин достижима малая область графа, то обход завершится быстрее. Для алгоритма в постановке задачи под номером 3 число стартовых вершин также определяет размер матрицы-фронта. Для обеспечения воспроизводимости результатов эксперимента множества стартовых вершин генерировалось при помощи функций [`generate_multiple_source`](https://jetbrains-research.github.io/CFPQ_Data/reference/graphs/generated/cfpq_data.graphs.utils.multiple_source_utils.html#cfpq_data.graphs.utils.multiple_source_utils.generate_multiple_source) и [`generate_multiple_source_percent`](https://jetbrains-research.github.io/CFPQ_Data/reference/graphs/generated/cfpq_data.graphs.utils.multiple_source_utils.html#cfpq_data.graphs.utils.multiple_source_utils.generate_multiple_source_percent) из библиотеки `CFPQ_Data` со значением параметра `seed`, равным `42`. Так как для вопроса 2 из цели данной работы требуется изучить влияние размера множества стартовых вершин, то значение параметра `set_size` определяется конкретным экспериментом.\n",
    "\n",
    "Напротив, множество финальных вершин влияет на быстродействие всех трёх алгоритмов тривиально и лишь своим размером: во всех случаях на завершающем этапе достигнутые вершины проверяются на принадлежность к множеству финальных вершин, что происходит через поиск по множеству. Поэтому для простоты в качестве множества финальных вершин выбиралось всё множество вершин графа."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Seed to generate the set of start nodes\n",
    "START_SET_SEED = 42"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Проводимые замеры\n",
    "\n",
    "Для ответа на каждый из поставленных вопросов нужно провести соответствующие замеры.\n",
    "\n",
    "#### Замеры для вопроса 1 (о выгоде от использования `pycubool`)\n",
    "\n",
    "Для выяснения того, при каких условиях использование `pycubool` даёт выигрыш в производительности против использования `scipy.sparse`, нужно выявить основные характеристики матриц, которые влияют на скорость выполнения данными библиотеками матричных операций.\n",
    "\n",
    "Естественно предположить, что это размер и степень разреженности (мера того, насколько матрица заполнена непустыми элементами): чем больше каждая из этих величин, тем больше нетривиальных операций нужно провести. Среди параметров графа и регулярного выражения на эти величины влияют, соответственно, число вершин (или же состояний конечного автомата, в случае регулярного выражения) и отношение числа рёбер (или же переходов) к числу вершин (или же состояний).\n",
    "\n",
    "Таким образом, для каждого из трёх алгоритмов следует для реализаций на `scipy.sparse` и `pycubool` замерить скорость их работы на всевозможных комбинациях выбранных графов и регулярных выражений с фиксированными стартовыми множествами и проследить зависимость разницы данных скоростей от указанных параметров.\n",
    "\n",
    "#### Замеры для вопроса 2 (о влиянии размера стартового множества на выгоду использования решений первой и третьей формулировок)\n",
    "\n",
    "Чтобы понять, при каких размерах множества стартовых вершинах, используя `pycubool`, эффективнее решать задачу в её первой постановке (то есть искать достижимые вершины среди всех пар вершин, а затем отбирать нужные), а при каких --- в третьей (то есть искать достижимые вершины лишь от стартовых вершин), требуется замерить скорость работы данных решений, реализованных на `pycubool`, при разных размерах стартового множества вершин.\n",
    "\n",
    "Таким образом, следует замерить скорость работы реализаций решения первой и третьей постановок задачи на `pycubool` на всевозможных комбинациях выбранных графов и регулярных выражений с различными размерами стартового множества и проследить, какая из реализаций будет показывать лучший результат в зависимости от взятых размеров.\n",
    "\n",
    "#### Замеры для вопроса 3 (о сравнении быстродействия решений второй и третьей формулировок)\n",
    "\n",
    "Чтобы выяснить, насколько решение задачи на `pycubool` в её второй формулировке (поиск для всего стартового множества вместе) быстрее решения в третьей (поиск для каждой стартовой вершины отдельно) при равных начальных условиях нужно замерить время их работы на одинаковых графах и регулярных запросах с равными стартовыми множествами.\n",
    "\n",
    "Таким образом, следует замерить скорость работы реализаций решения второй и третьей постановок задачи на `pycubool` на всевозможных комбинациях выбранных графов и регулярных выражений с некоторыми фиксированным стартовыми множествами и сравнить результаты.\n",
    "\n",
    "#### Итог по необходимым замерам\n",
    "\n",
    "Итого, для ответа на все три вопроса достаточно провести следующие замеры времени выполнения:\n",
    "\n",
    "- на всевозможных комбинациях выбранных графов и регулярных выражений с фиксированными стартовыми множествами --- для всех реализаций;\n",
    "- на всевозможных комбинациях выбранных графов, регулярных выражений и размеров стартовых множеств --- для реализаций первой и третьей формулировок задачи на `pycubool`.\n",
    "\n",
    "Для небольших графов (с числом вершин не более 10000) время работы замеряется пять раз подряд, для больших (с числом вершин более 10000) --- два раза. Замеры проводится при помощи [`time.process_time`](https://docs.python.org/3/library/time.html#time.process_time), подсчитывается среднее время исполнения и стандартное отклонение через функции модуля `statistics`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Percent of nodes to take as start in the fixed-start experiment\n",
    "FIXED_START_SIZE = 50\n",
    "# Sizes of start sets in varied-start experiment\n",
    "VARIED_START_SIZES = [50, 100, 200, 400, 800, 1500, 3000, 6000, 10000]\n",
    "# Number of consecutive times each measurement is run for small graphs\n",
    "RUN_TIMES_SMALL = 5\n",
    "# Number of consecutive times each measurement is run for huge graphs\n",
    "RUN_TIMES_HUGE = 2\n",
    "\n",
    "\n",
    "# Determines which graphs are considered small\n",
    "def is_small(graph: nx.Graph) -> bool:\n",
    "    return graph.number_of_edges() <= 10000"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ниже представлен код для проведения описанных замеров."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from time import process_time\n",
    "from statistics import fmean\n",
    "from statistics import stdev\n",
    "from typing import Callable\n",
    "from cfpq_data import generate_multiple_source\n",
    "\n",
    "\n",
    "def run_timed(f: Callable, run_times: int) -> tuple[float, float]:\n",
    "    times = []\n",
    "    for _ in range(run_times):\n",
    "        start = process_time()\n",
    "        f()\n",
    "        finish = process_time()\n",
    "        times.append(finish - start)\n",
    "\n",
    "    m = fmean(times)\n",
    "    sd = stdev(times, m)\n",
    "    return m, sd\n",
    "\n",
    "\n",
    "def run_fixed_starts():\n",
    "    results = []\n",
    "\n",
    "    for g_i, graph in enumerate(GRAPHS, start=1):\n",
    "        regexes = create_regexes_for_graph(graph)\n",
    "        run_times = RUN_TIMES_SMALL if is_small(graph) else RUN_TIMES_HUGE\n",
    "        starts = generate_multiple_source(graph, FIXED_START_SIZE, seed=START_SET_SEED)\n",
    "        print(f\"########## Graph {graph.name} ({g_i} / {len(GRAPHS)}, run times = {run_times}) ##########\")\n",
    "\n",
    "        graph_results = []\n",
    "        for r_i, regex in enumerate(regexes, start=1):\n",
    "            print(f\"========== Regex #{r_i} ==========\")\n",
    "\n",
    "            print(\"by_tensor\", end=\" \")\n",
    "            by_tensor = run_timed(\n",
    "                lambda: rpq_by_tensor(graph, regex, starts=starts),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            print(\"by_bfs_all\", end=\" \")\n",
    "            by_bfs_all = run_timed(\n",
    "                lambda: rpq_by_bfs(graph, regex, starts=starts, mode=BfsMode.FIND_COMMON_REACHABLE_SET),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            print(\"by_bfs_each\", end=\" \")\n",
    "            by_bfs_each = run_timed(\n",
    "                lambda: rpq_by_bfs(graph, regex, starts=starts, mode=BfsMode.FIND_REACHABLE_FOR_EACH_START),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            print(\"by_tensor_cuda\", end=\" \")\n",
    "            by_tensor_cuda = run_timed(\n",
    "                lambda: rpq_by_tensor_cuda(graph, regex, starts=starts),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            print(\"by_bfs_all_cuda\", end=\" \")\n",
    "            by_bfs_all_cuda = run_timed(\n",
    "                lambda: rpq_by_bfs_cuda(graph, regex, starts=starts, mode=BfsMode.FIND_COMMON_REACHABLE_SET),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            print(\"by_bfs_each_cuda\")\n",
    "            by_bfs_each_cuda = run_timed(\n",
    "                lambda: rpq_by_bfs_cuda(graph, regex, starts=starts, mode=BfsMode.FIND_REACHABLE_FOR_EACH_START),\n",
    "                run_times=run_times,\n",
    "            )\n",
    "\n",
    "            graph_results.append(\n",
    "                ((by_tensor, by_bfs_all, by_bfs_each), (by_tensor_cuda, by_bfs_all_cuda, by_bfs_each_cuda))\n",
    "            )\n",
    "\n",
    "        results.append(graph_results)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def run_varied_starts():\n",
    "    results = []\n",
    "\n",
    "    for g_i, graph in enumerate(GRAPHS, start=1):\n",
    "        run_times = RUN_TIMES_SMALL if is_small(graph) else RUN_TIMES_HUGE\n",
    "        regexes = create_regexes_for_graph(graph)\n",
    "        print(f\"########## Graph {graph.name} ({g_i} / {len(GRAPHS)}, run times = {run_times}) ##########\")\n",
    "\n",
    "        graph_results = []\n",
    "        for r_i, regex in enumerate(regexes, start=1):\n",
    "            print(f\"========== Regex #{r_i} ==========\")\n",
    "\n",
    "            regex_results = []\n",
    "            for start_size in VARIED_START_SIZES:\n",
    "                if start_size > graph.number_of_nodes():\n",
    "                    continue\n",
    "                starts = generate_multiple_source(graph, start_size, seed=START_SET_SEED)\n",
    "                print(f\"********** Start size #{start_size} **********\")\n",
    "\n",
    "                print(\"by_tensor_cuda\", end=\" \")\n",
    "                by_tensor_cuda = run_timed(\n",
    "                    lambda: rpq_by_tensor_cuda(graph, regex, starts=starts),\n",
    "                    run_times=run_times,\n",
    "                )\n",
    "\n",
    "                print(\"by_bfs_each_cuda\")\n",
    "                by_bfs_each_cuda = run_timed(\n",
    "                    lambda: rpq_by_bfs_cuda(graph, regex, starts=starts, mode=BfsMode.FIND_REACHABLE_FOR_EACH_START),\n",
    "                    run_times=run_times,\n",
    "                )\n",
    "\n",
    "                regex_results.append((by_tensor_cuda, by_bfs_each_cuda))\n",
    "\n",
    "            graph_results.append(regex_results)\n",
    "\n",
    "        results.append(graph_results)\n",
    "\n",
    "    return results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Результаты эксперимента\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Заключение\n",
    "\n",
    "TODO"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
